{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8915e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Proxy Provider Reviews - Proxyway</title>\n",
      "title\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "r = requests.get('https://proxyway.com/reviews') # Melakukan permintaan HTTP GET ke URL\n",
    "soup = BeautifulSoup(r.content, 'html.parser') # Membuat objek BeautifulSoup dari konten response yang diterima (r.content)\n",
    "print(soup.title) # Mengambil elemen <title> dari halaman web yang telah di-parse dan mencetaknya\n",
    "print(soup.title.name) # Mengambil nama tag dari elemen <title> dan mencetaknya\n",
    "print(soup.title.parent.name) # Mengambil nama tag dari elemen induk (parent) dari elemen <title> dan mencetaknya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c259a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sub Titles Page: 1 \n",
      "\n",
      "1 Smartproxy\n",
      "2 Oxylabs\n",
      "3 Bright Data\n",
      "4 SOAX\n",
      "5 NetNut\n",
      "6 Proxydrop\n",
      "7 Rayobyte\n",
      "8 GeoSurf\n",
      "9 Zyte (Crawlera)\n",
      "10 IPRoyal\n",
      "11 PacketStream\n",
      "12 Shifter\n",
      "\n",
      "\n",
      "Sub Titles Page: 2 \n",
      "\n",
      "1 Multilogin\n",
      "2 GoLogin\n",
      "3 Infatica\n",
      "4 Storm Proxies\n",
      "5 SERPMaster\n",
      "6 AstroProxy\n",
      "7 InstantProxies\n",
      "8 RSocks\n",
      "9 Iced Out Proxies\n",
      "10 Brazy Kicks\n",
      "11 The Proxy Store\n",
      "12 LimeProxies\n",
      "\n",
      "\n",
      "Sub Titles Page: 3 \n",
      "\n",
      "1 HighProxies\n",
      "2 MyPrivateProxy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URL = 'https://proxyway.com/reviews' # situs web yang akan discrape\n",
    "\n",
    "for page in range(1, 4): # Melakukan iterasi dari halaman 1 hingga 3\n",
    "    print(\"\\n\")\n",
    "    print(\"Sub Titles Page:\", page, \"\\n\")\n",
    "\n",
    "    req = requests.get(f'{URL}/page/{page}')  # Mengirim permintaan GET ke URL halaman yang sesuai dengan nomor halaman saat ini\n",
    "    soup = bs(req.text, 'html.parser')  # Membuat objek BeautifulSoup untuk mem-parse HTML dari response\n",
    "\n",
    "    titles = soup.find_all('h3', class_='archive-list__title')  # Mencari semua elemen <h3> dengan kelas CSS 'archive-list__title' yang mewakili judul-judul\n",
    "\n",
    "    for i, title in enumerate(titles): # Melakukan iterasi melalui setiap judul dalam titles, sambil melacak indeksnya menggunakan fungsi enumerate()\n",
    "        print(f\"{i+1} {title.text}\")  # Mencetak nomor judul dan teks judul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c849f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv # Mengimpor library csv untuk menulis data ke file CSV\n",
    "\n",
    "URL = 'https://proxyway.com/reviews/page/' # situs web yang akan discrape\n",
    "\n",
    "# Mendefinisikan dua list kosong untuk menyimpan data judul dari halaman-halaman yang di-scrape\n",
    "titles_list = []\n",
    "title_page3 = []\n",
    "\n",
    "for page in range(1, 4): # Melakukan iterasi dari halaman 1 hingga 3 \n",
    "    req = requests.get(URL + str(page) + '/') # Mengirim permintaan HTTP GET ke URL \n",
    "    soup = bs(req.text, 'html.parser') # Membuat objek BeautifulSoup dari teks respons yang diterima (req.text)\n",
    "    \n",
    "    titles = soup.find_all('h3', attrs={'class', 'archive-list__title'}) # Mencari semua elemen <h3> dengan atribut class yang sesuai dengan 'archive-list__title'\n",
    "    \n",
    "    # Mengiterasi melalui setiap judul dalam titles, sambil melacak nomor judul dengan variabel count\n",
    "    count = 1\n",
    "    for title in titles:\n",
    "        d = {}\n",
    "        d['Page Number'] = f'Page {page}'\n",
    "        d['Title Number'] = f'Title {count}'\n",
    "        d['Title Name'] = title.text\n",
    "        count += 1\n",
    "        titles_list.append(d)\n",
    "    \n",
    "    # Memeriksa apakah halaman saat ini adalah halaman ke 3 atau bukan\n",
    "        if page == 3 :\n",
    "            count = 1\n",
    "            for title in titles:\n",
    "                e = {}\n",
    "                e['Page Number'] = f'Page {page}'\n",
    "                e['Title Number'] = f'Title {count}'\n",
    "                e['Title Name'] = title.text\n",
    "                count += 1\n",
    "                title_page3.append(d)\n",
    "        \n",
    "titles_all = titles_list + title_page3 # Menggabungkan list titles_list dan title_page3 menjadi satu list\n",
    "\n",
    "filename = 'simpan ke csv.csv' # Menyimpan nama file CSV yang akan digunakan untuk menyimpan data\n",
    "with open (filename, 'w', newline='') as f: # Membuka file CSV dengan mode write ('w')\n",
    "    w = csv.DictWriter(f,['Page Number', 'Title Number', 'Title Name']) # Membuat objek DictWriter yang akan menulis data sebagai dictionary ke file CSV\n",
    "    w.writeheader() # Menulis baris header (nama kolom) ke file CSV\n",
    "    \n",
    "    w.writerows(titles_all) # Menulis semua baris data (judul-judul) dari titles_all ke file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4f616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
